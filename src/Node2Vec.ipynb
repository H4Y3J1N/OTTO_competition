{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import polars as pl\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node2vec 모델에서 embedding을 추출하는 것부터 시작합니다.\n",
    "# 학습을 위해 그래프 데이터 추출\n",
    "\n",
    "#이전 세션(session)의 aid 값을 포함하는 그래프 데이터 저장\n",
    "train_df = pl.read_parquet('/home/conceptelling/data/train.parquet', columns=['session', 'aid'], low_memory=True)\n",
    "df = train_df.with_columns(pl.col(\"aid\").shift(periods=1).over(\"session\")\n",
    "                              #.cast(pl.Int32)\n",
    "                              #.fill_null(pl.col(\"aid\"))\n",
    "                              .alias(\"prev_aid\"))\n",
    "\n",
    "# 세션별 첫 번째 행 제거 (prev_aid가 없는 경우)\n",
    "df = df.filter(pl.col('prev_aid').is_not_null())\n",
    "\n",
    "edges_torch_T = torch.tensor(np.transpose(df[['prev_aid', 'aid']].to_numpy()), dtype=torch.long)\n",
    "# torch.save(edges_torch_T, \"all_graph_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(edge_index=edges_torch_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec_train(embedding_dim, walk_length, context_size, walks_per_node, p, q, lr):\n",
    "    model = Node2Vec(data.edge_index, embedding_dim=embedding_dim,\n",
    "                     walk_length=walk_length,\n",
    "                     context_size=context_size,\n",
    "                     walks_per_node=walks_per_node,\n",
    "                     num_negative_samples=2,\n",
    "                     p=p, q=q,\n",
    "                     sparse=True).to(device)\n",
    "\n",
    "    loader = model.loader(batch_size=1024, shuffle=True, num_workers=6)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=lr)\n",
    "\n",
    "    num_epochs = 2\n",
    "    total_loss = 0\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return model, total_loss / len(loader)\n",
    "\n",
    "space = [Integer(32, 128, name='embedding_dim'),\n",
    "         Integer(10, 20, name='walk_length'),\n",
    "         Integer(5, 10, name='context_size'),\n",
    "         Integer(10, 20, name='walks_per_node'),\n",
    "         Real(0.5, 2, name='p'),\n",
    "         Real(0.5, 2, name='q'),\n",
    "         Real(1e-4, 1e-3, \"log-uniform\", name='lr')]\n",
    "\n",
    "# 진행상황 출력\n",
    "from tqdm.auto import tqdm as tqdm_auto\n",
    "tqdm_auto_pbar = tqdm_auto(total=15, desc=\"Bayesian optimization\")\n",
    "\n",
    "models_and_losses = []\n",
    "\n",
    "# objective 중복 평가를 방지하기 위해 코드 개선. 그런데 대충 15번 정도면 찾아내는 것 같다.\n",
    "evaluated_points = set()\n",
    "@use_named_args(space)\n",
    "def objective(embedding_dim, walk_length, context_size, walks_per_node, p, q, lr):\n",
    "    point = (embedding_dim, walk_length, context_size, walks_per_node, p, q, lr)\n",
    "    if point in evaluated_points:\n",
    "        return 1e9\n",
    "    else:\n",
    "        evaluated_points.add(point)\n",
    "    tqdm_auto_pbar.update(1)\n",
    "    if walk_length < context_size:\n",
    "        return 1e9\n",
    "    model, loss = node2vec_train(embedding_dim, walk_length, context_size, walks_per_node, p, q, lr)\n",
    "    models_and_losses.append((model, loss))\n",
    "    return loss\n",
    "\n",
    "\n",
    "result = gp_minimize(objective, space, n_calls=15, random_state=0)\n",
    "tqdm_auto_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters found: \", result.x)\n",
    "print(\"Minimum loss: \", result.fun)\n",
    "\n",
    "best_params = {'embedding_dim': result.x[0], 'walk_length': result.x[1], 'context_size': result.x[2],\n",
    "               'walks_per_node': result.x[3], 'p': result.x[4], 'q': result.x[5], 'lr': result.x[6]}\n",
    "\n",
    "best_model, best_loss = min(models_and_losses, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec(embedding_dim, walk_length, context_size, walks_per_node, p, q, lr):\n",
    "    model = Node2Vec(data.edge_index, embedding_dim=embedding_dim,\n",
    "                     walk_length=walk_length,\n",
    "                     context_size=context_size,\n",
    "                     walks_per_node=walks_per_node,\n",
    "                     num_negative_samples=2,\n",
    "                     p=p, q=q,\n",
    "                     sparse=True).to(device)\n",
    "\n",
    "    loader = model.loader(batch_size=1024, shuffle=True, num_workers=6)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=lr)\n",
    "\n",
    "    num_epochs = 5\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        total_loss = 0  \n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}: Loss {total_loss}\") \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best param으로 모델 학습\n",
    "trained_model = node2vec(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 추출\n",
    "node_embeddings = trained_model.embedding.weight.cpu().detach().numpy()\n",
    "embedding_save_path = \"node_embeddings.npy\"\n",
    "np.save(embedding_save_path, node_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
