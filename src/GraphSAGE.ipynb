{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wikid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Input Data\n",
    "### Node Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node2vec embeddings and node features\n",
    "node2vec_embeddings = np.load(\"./output/node_embeddings.npy\")\n",
    "aid_features = pl.read_parquet(\"./data/aid_features.parquet\")\n",
    "aid_features_agg = pl.read_parquet(\"./data/aid_features_agg.parquet\")\n",
    "aid_features_all = aid_features.join(aid_features_agg, on=\"aid\", how=\"inner\").drop(\"aid\").to_numpy()\n",
    "\n",
    "# Scaling node2vec embeddings and node features separately\n",
    "scaler_node2vec = StandardScaler()\n",
    "scaled_node2vec_embeddings = scaler_node2vec.fit_transform(node2vec_embeddings)\n",
    "\n",
    "scaler_features = StandardScaler()\n",
    "scaled_aid_features_all = scaler_features.fit_transform(aid_features_all)\n",
    "\n",
    "# Concatenate node2vec embeddings and node features\n",
    "features_and_embeddings = np.concatenate((scaled_node2vec_embeddings, scaled_aid_features_all), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del node2vec_embeddings,aid_features_all,aid_features_agg,aid_features\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 로드하고, 'user_id'와 'item_id' 열을 기준으로 구매 횟수를 집계합니다.\n",
    "data = pd.read_parquet('./data/train.parquet')\n",
    "edge_weights = data.groupby(['session', 'aid']).size().reset_index(name='weight')\n",
    "\n",
    "# 간선 목록 및 가중치 목록을 추출합니다.\n",
    "edge_list = edge_weights[['session', 'aid']].values.tolist()\n",
    "edge_weights_list = edge_weights['weight'].values.tolist()\n",
    "\n",
    "# 간선 목록 및 가중치 목록을 텐서로 변환합니다.\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_weights_list, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbor Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gSAGE_loader = NeighborLoader(\n",
    "    graph_data,\n",
    "    # 각 레이어에서 샘플링할 이웃 노드의 수를 나열한 리스트\n",
    "    num_neighbors=[10,10],\n",
    "    # 한 번에 처리할 노드의 수를 결정하는 배치 크기\n",
    "    batch_size=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 정보를 사용하도록 수정한 GraphSAGE 코드\n",
    "\n",
    "class WeightedSAGEConv(SAGEConv):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(WeightedSAGEConv, self).__init__(in_channels, out_channels, **kwargs)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = (x[0], x[1])\n",
    "        return super(WeightedSAGEConv, self).forward(x, edge_index, edge_weight)\n",
    "\n",
    "    \n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.convs.append(WeightedSAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(WeightedSAGEConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        # Output layer\n",
    "        self.convs.append(WeightedSAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, edge_attr, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target node features\n",
    "            x = self.convs[i]((x, x_target), edge_index, edge_attr)\n",
    "\n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "    def inference(self, x_all, subgraph_loader, device):\n",
    "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, edge_attr, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = self.convs[i]((x, x_target), edge_index, edge_attr)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "        return x_all\n",
    "    \n",
    "    \n",
    "out_channels = 32\n",
    "num_features = data.x.shape[1]\n",
    "hidden_channels = 64\n",
    "num_layers = 2\n",
    "model = GAE(GraphSAGE(num_features, hidden_channels, out_channels, num_layers)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005,weight_decay=1e-5)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAE로 그래서 뭘 하려는 건지 원 캐글코드를 먼저 끝까지 한번 훑어야 할 듯\n",
    "그 다음에 내 학습코드 방향 잡을 수 있음 (캐글따라감/임의수정 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터 튜닝 코드로 개선 \n",
    "# y 라벨로 학습 할 수 있는 건지 다시 체크 (그렇게 학습시켜야 할 것 같은데ㅠㅠ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
